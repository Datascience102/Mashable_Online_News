---
title: "Mashable.com - A Process to Predict Online News Popularity"
author: "Abhinandan Saini"
output:
  html_document:
    css: ../../AnalyticsStyles/default.css
    theme: paper
    toc: no
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    includes:
      in_header: ../../AnalyticsStyles/default.sty
always_allow_html: yes
---

# The Business Decision 

This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the number of shares in social networks, i.e. how popular any given article is. The dataset is publicly available at [University of California Irvine Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Online+News+Popularity)

[Mashable Inc.](http://www.mashable.com) is a digital media website founded in 2005. It has been described as a "one stop shop" for social media. As of November 2015, it has over 6,000,000 Twitter followers and over 3,200,000 fans on Facebook.

```{r setuplibraries, echo=FALSE, message=FALSE}
suppressWarnings(source("../INSEADAnalytics/AnalyticsLibraries/library.R"))
suppressWarnings(source("../INSEADAnalytics/AnalyticsLibraries/heatmapOutput.R"))
# Package options
suppressWarnings(ggthemr('fresh'))  # ggplot theme
opts_knit$set(progress=FALSE, verbose=FALSE)
opts_chunk$set(echo=FALSE, fig.align="center", fig.width=10, fig.height=6.35, results="asis")
options(knitr.kable.NA = '')
```

<hr>\clearpage

# The Data

First we load the data to use (see the raw .Rmd file to change the data file as needed):

```{r setupdata1E, echo=FALSE, tidy=TRUE}
# Please ENTER the name of the file with the data used. The file should be a .csv with one row per observation (e.g. person) and one column per attribute. Do not add .csv at the end, make sure the data are numeric.
datafile_name = "data/OnlineNewsPopularity.csv"

# Please enter the minimum number below which you would like not to print - this makes the readability of the tables easier. Default values are either 10e6 (to print everything) or 0.5. Try both to see the difference.
MIN_VALUE = 0.5

# Please enter the maximum number of observations to show in the report and slides. 
# DEFAULT is 10. If the number is large the report may be slow.
max_data_report = 10
```

```{r}
ProjectData <- read.csv(datafile_name)
ProjectData <- data.matrix(ProjectData) 
ProjectData_INITIAL <- ProjectData

```

**Attribute Information in Dataset are as follows:**

0. **url:** URL of the article (non-predictive)
1. **timedelta:** Days between the article publication and the dataset acquisition (non-predictive)
2. **n_tokens_title:** Number of words in the title 
3. **n_tokens_content** Number of words in the content 
4. **n_unique_tokens:** Rate of unique words in the content 
5. **n_non_stop_unique_tokens:** Rate of unique non-stop words in the content 
6. **num_hrefs:** Number of links 
7. **num_self_hrefs:** Number of links to other articles published by Mashable 
8. **num_imgs:** Number of images 
9. **num_videos:** Number of videos 
10. **average_token_length:** Average length of the words in the content 
11. **num_keywords:** Number of keywords in the metadata 
12. **self_reference_min_shares:** Min. shares of referenced articles in Mashable 
13. **self_reference_max_shares:** Max. shares of referenced articles in Mashable 
14. **self_reference_avg_sharess:** Avg. shares of referenced articles in Mashable 
15. **global_subjectivity:** Text subjectivity 
16. **global_sentiment_polarity:** Text sentiment polarity 
17. **global_rate_positive_words:** Rate of positive words in the content 
18. **global_rate_negative_words:** Rate of negative words in the content 
19. **rate_positive_words:** Rate of positive words among non-neutral tokens 
20. **rate_negative_words:** Rate of negative words among non-neutral tokens 
21. **title_subjectivity:** Title subjectivity 
22. **title_sentiment_polarity:** Title polarity 
23. **abs_title_subjectivity:** Absolute subjectivity level 
24. **abs_title_sentiment_polarity:** Absolute polarity level 
25. **shares:** Number of shares (target)

**Stop Words** usually refer to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools. For some search engines, these are some of the most common, short function words, such as the, is, at, which, and on.

<hr>\clearpage

# Dimensionality Reduction

```{r setupfactor, echo=FALSE, tidy=TRUE}
# Please ENTER then original raw attributes to use. 
# Please use numbers, not column names, e.g. c(1:5, 7, 8) uses columns 1,2,3,4,5,7,8
factor_attributes_used = c(3:25)

# Please ENTER the selection criterions for the factors to use. 
# Choices: "eigenvalue", "variance", "manual"
factor_selectionciterion = "eigenvalue"

# Please ENTER the desired minumum variance explained 
# (Only used in case "variance" is the factor selection criterion used). 
minimum_variance_explained = 65  # between 1 and 100

# Please ENTER the number of factors to use 
# (Only used in case "manual" is the factor selection criterion used).
manual_numb_factors_used = 8

# Please ENTER the rotation eventually used (e.g. "none", "varimax", "quatimax", "promax", "oblimin", "simplimax", and "cluster" - see help(principal)). Default is "varimax"
rotation_used = "varimax"

```

```{r}
factor_attributes_used <- intersect(factor_attributes_used, 1:ncol(ProjectData))
ProjectDatafactor <- ProjectData[,factor_attributes_used]
ProjectDatafactor <- ProjectData <- data.matrix(ProjectDatafactor)
```

## Steps 1-2: Check the Data 

Here is a sample of the first 250 rows of the Dataset:

```{r}
options(warn=-1)
library(googleVis)
library(dplyr)
library(lattice)
library(ggplot2)

local_directory <- getwd()
if (!exists("gadata1")) 
  gadata1 <- within(read.csv(paste(local_directory,"data/OnlineNewsPopularity.csv", sep="/")),rm("X"))
t1 <- gvisTable(gadata1[1:250,],options = list(showRowNumber = FALSE, width = 800, height = min(400,27*(nrow(gadata1) + 1)), allowHTML = TRUE, page = 'disable'))
print(t1,'chart')
```


The data we use here have the following descriptive statistics: 

```{r}
iprint.df(round(my_summary(ProjectDatafactor), 2))
```

The data is Scaled and summary statistics are reprinted:

```{r, echo=FALSE, tidy=TRUE}
ProjectDatafactor_scaled=apply(ProjectDatafactor, 2, function(r) {if (sd(r)!=0) res=(r-mean(r))/sd(r) else res=0*r; res})
```

```{r}
iprint.df(round(my_summary(ProjectDatafactor_scaled), 2))
```


## Step 3: Check Correlations

```{r}
thecor = round(cor(ProjectDatafactor),2)
iprint.df(round(thecor,2), scale=TRUE)
```


## Step 4: Choose number of factors


```{r}
# Here is how the `principal` function is used 
UnRotated_results<-principal(ProjectDatafactor, nfactors=ncol(ProjectDatafactor), rotate="none",score=TRUE)
UnRotated_factors<-round(UnRotated_results$loadings,2)
UnRotated_factors<-as.data.frame(unclass(UnRotated_factors))
colnames(UnRotated_factors)<-paste("Comp",1:ncol(UnRotated_factors),sep="")
```

```{r}
# Here is how we use the `PCA` function 
Variance_Explained_Table_results<-PCA(ProjectDatafactor, graph=FALSE)
Variance_Explained_Table<-Variance_Explained_Table_results$eig
Variance_Explained_Table_copy<-Variance_Explained_Table

rownames(Variance_Explained_Table) <- paste("Component", 1:nrow(Variance_Explained_Table), sep=" ")
colnames(Variance_Explained_Table) <- c("Eigenvalue", "Pct of explained variance", "Cumulative pct of explained variance")
```

After running the Principal Component Analysis, we loook at the **variance explained** as well as the **eigenvalues** to choose the relevant number of factors:

```{r}
iprint.df(round(Variance_Explained_Table, 2))
```

```{r}
eigenvalues  <- Variance_Explained_Table[, "Eigenvalue"]
df           <- cbind(as.data.frame(eigenvalues), c(1:length(eigenvalues)), rep(1, length(eigenvalues)))
colnames(df) <- c("eigenvalues", "components", "abline")
iplot.df(melt(df, id="components"))
```

Based on the Principal Component Analysis, 6 factors out of the 23 are chosen.

## Step 5: Interpret the factors


```{r}
if (factor_selectionciterion == "eigenvalue")
  factors_selected = sum(Variance_Explained_Table_copy[,1] >= 1)
if (factor_selectionciterion == "variance")
  factors_selected = 1:head(which(Variance_Explained_Table_copy[,"cumulative percentage of variance"]>= minimum_variance_explained),1)
if (factor_selectionciterion == "manual")
  factors_selected = manual_numb_factors_used
```

We check the correlation of each of these six factors with the rest of the attributes.

```{r}
Rotated_results<-principal(ProjectDatafactor, nfactors=max(factors_selected), rotate=rotation_used,score=TRUE)
Rotated_factors<-round(Rotated_results$loadings,2)
Rotated_factors<-as.data.frame(unclass(Rotated_factors))
colnames(Rotated_factors)<-paste("Comp.",1:ncol(Rotated_factors),sep="")

sorted_rows <- sort(Rotated_factors[,1], decreasing = TRUE, index.return = TRUE)$ix
Rotated_factors <- Rotated_factors[sorted_rows,]

iprint.df(Rotated_factors, scale=TRUE)
```

To better visualize and interpret the factors we often "suppress" loadings with small values, e.g. with absolute values smaller than 0.5. In this case our factors look as follows after suppressing the small numbers:

```{r}
Rotated_Factors_thres <- Rotated_factors
Rotated_Factors_thres[abs(Rotated_Factors_thres) < MIN_VALUE]<-NA
colnames(Rotated_Factors_thres)<- colnames(Rotated_factors)
rownames(Rotated_Factors_thres)<- rownames(Rotated_factors)

iprint.df(Rotated_Factors_thres, scale=TRUE)
```


## Step 6:  Save factor scores

We can now either replace all initial variables used in this part with one of the initial variables for each of the selected factors in order to represent that factor. Here is how the factor scores  are for the first few respondents:

```{r}
NEW_ProjectData <- round(Rotated_results$scores[,1:factors_selected,drop=F],2)
colnames(NEW_ProjectData)<-paste("DV (Factor)",1:ncol(NEW_ProjectData),sep=" ")

iprint.df(t(head(NEW_ProjectData, 10)), scale=TRUE)
```

Where,

**DV (Factor) 1:** Rate of unique non-stop words in the content

**DV (Factor) 2:** Rate of negative (or positive) words in the content

**DV (Factor) 3:** Avg. shares of referenced articles in Mashable

**DV (Factor) 4:** Number of words in the content

**DV (Factor) 5:** Absolute polarity level in title

**DV (Factor) 6:** Number of videos in the article

<hr>\clearpage

By focusing on these six factors, Mashable should be able to better predict whether an article will be shared on social media. Moreover, Mashable can potentially increase the number of shares for each article by setting the value of each of these attributes such that it maximizes the chance that a reader will share that article. 


<hr>\clearpage

# Cluster Analysis and Segmentation

```{r setup, echo=FALSE, message=FALSE}

# Please ENTER the name of the file with the data used. The file should contain a matrix with one row per observation (e.g. person) and one column per attribute. THE NAME OF THIS MATRIX NEEDS TO BE ProjectData (otherwise you will need to replace the name of the ProjectData variable below with whatever your variable name is, which you can see in your Workspace window after you load your file)
#datafile_name="Boats" # do not add .csv at the end! make sure the data are numeric!!!! check your file!
datafile_name3="OnlineNewsPopularity_forSegmentation" # do not add .csv at the end! make sure the data are numeric!!!! check your file!

# Please ENTER then original raw attributes to use for the segmentation (the "segmentation attributes")
# Please use numbers, not column names, e.g. c(1:5, 7, 8) uses columns 1,2,3,4,5,7,8
segmentation_attributes_used = c(6, 21, 15, 4, 25, 10) 

# Please ENTER the number of clusters to eventually use for this report
numb_clusters_used = 3 # for boats possibly use 5, for Mall_Visits use 3

# Please enter the method to use for the segmentation:
profile_with = "kmeans" #  "hclust" or "kmeans"

# Please ENTER the distance metric eventually used for the clustering in case of hierarchical clustering 
# (e.g. "euclidean", "maximum", "manhattan", "canberra", "binary" or "minkowski" - see help(dist)). 
# DEFAULT is "euclidean"
distance_used="manhattan"

# Please ENTER the hierarchical clustering method to use (options are:
# "ward", "single", "complete", "average", "mcquitty", "median" or "centroid")
# DEFAULT is "ward"
hclust_method = "ward.D"

# Please ENTER the kmeans clustering method to use (options are:
# "Hartigan-Wong", "Lloyd", "Forgy", "MacQueen"
# DEFAULT is "Lloyd"
kmeans_method = "Lloyd"

# Please enter the minimum number below which you would like not to print - this makes the readability of the tables easier. Default values are either 10e6 (to print everything) or 0.5. Try both to see the difference.
MIN_VALUE=0.5

# Please enter the maximum number of observations to show in the report and slides 
# (DEFAULT is 50. If the number is large the report and slides may not be generated - very slow or will crash!!)
max_data_report = 10 # can also chance in server.R
```

```{r}
# this loads the selected data: DO NOT EDIT THIS LINE
ProjectData3 <- read.csv(paste(paste(local_directory, "data", sep="/"), paste(datafile_name3,"csv", sep="."), sep = "/")) # this contains only the matrix ProjectData
ProjectData3=data.matrix(ProjectData3) 
if (datafile_name3 == "OnlineNewsPopularity_forSegmentation")
  colnames(ProjectData3)<-gsub("\\."," ",colnames(ProjectData3))

segmentation_attributes_used = unique(sapply(segmentation_attributes_used,function(i) min(ncol(ProjectData3), max(i,1))))

ProjectData3_segment=ProjectData3[,segmentation_attributes_used]
# this is the file name where the CLUSTER_IDs of the observations will be saved
cluster_file = paste(paste(local_directory,"data", sep="/"),paste(paste(datafile_name3,"cluster", sep="_"), "csv", sep="."), sep="/")
```

## The Data

```{r}
# let's make the data into data.matrix classes so that we can easier visualize them
ProjectData3_segment = data.matrix(ProjectData3_segment)
```

There are a total of 39,565 URLs in the data. Here are the responses for the first `r min(max_data_report,nrow(ProjectData))` URLs based on the six factors we chose in the Dimensionality Reduction stage:

```{r}
knitr::kable(round(head(ProjectData3_segment, max_data_report), 2))
```


## Summary Statistics

```{r}
knitr::kable(round(my_summary(ProjectData3_segment),2))
```

## Scaled Summary Statistics

```{r, echo=FALSE, tidy=TRUE}
ProjectData3_scaled=apply(ProjectData3_segment,2, function(r) {if (sd(r)!=0) res=(r-mean(r))/sd(r) else res=0*r; res})
```

```{r}
knitr::kable(round(my_summary(ProjectData3_scaled),2))
```

## Using Kmeans Clustering

We use Kmeans clustering to look for 3 clusters using the Lloyd Kmeans method. Here is the cluster membership for the first 10 URLs:

```{r}
kmeans_clusters <- kmeans(ProjectData3_scaled,centers= numb_clusters_used, iter.max=2000, algorithm=kmeans_method)

ProjectData_with_kmeans_membership <- cbind(1:length(kmeans_clusters$cluster),kmeans_clusters$cluster)
colnames(ProjectData_with_kmeans_membership)<-c("Observation Number","Cluster_Membership")

knitr::kable(round(head(ProjectData_with_kmeans_membership, max_data_report), 2))
```

## Interpreting the Segments 

We compare the average responses for each segment with the population average:

```{r}
cluster_memberships_kmeans <- kmeans_clusters$cluster 
cluster_ids_kmeans <- unique(cluster_memberships_kmeans)

if (profile_with == "hclust"){
  cluster_memberships <- cluster_memberships_hclust
  cluster_ids <-  cluster_ids_hclust  
}
if (profile_with == "kmeans"){
  cluster_memberships <- cluster_memberships_kmeans
  cluster_ids <-  cluster_ids_kmeans
}

# SAVE THE DATA in the cluster file
NewData = matrix(cluster_memberships,ncol=1)
write.csv(NewData,file=cluster_file)

population_average = matrix(apply(ProjectData3_segment, 2, mean), ncol=1)
colnames(population_average) <- "Population"
Cluster_Profile_mean <- sapply(sort(cluster_ids), function(i) apply(ProjectData3_segment[(cluster_memberships==i), ], 2, mean))
if (ncol(ProjectData3_segment) <2)
  Cluster_Profile_mean=t(Cluster_Profile_mean)
colnames(Cluster_Profile_mean) <- paste("Segment", 1:length(cluster_ids), sep=" ")
cluster.profile <- cbind (population_average,Cluster_Profile_mean)

knitr::kable(round(cluster.profile, 2))
```

To get a better picture of the magnitude of the differences, the segments are scaled:

```{r}
population_average_matrix <- population_average[,"Population",drop=F] %*% matrix(rep(1,ncol(Cluster_Profile_mean)),nrow=1)
cluster_profile_ratios <- (ifelse(population_average_matrix==0, 0,Cluster_Profile_mean/population_average_matrix-1))
colnames(cluster_profile_ratios) <- paste("Segment", 1:ncol(cluster_profile_ratios), sep=" ")
rownames(cluster_profile_ratios) <- colnames(ProjectData3_scaled)
## printing the result in a clean-slate table
knitr::kable(round(cluster_profile_ratios, 2))
```

Although there seems to be some differences between the three segments, after running the Kmeans clustering test numerous times it becomes clear that the segments are not very distinct. In other words, the distance between each segment is not that large. As such, it does not make much sense to segment the URLs based on these factors. 

